{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10334248,"sourceType":"datasetVersion","datasetId":6398889}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom torchtext.data.utils import get_tokenizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_fscore_support, confusion_matrix\nimport pandas as pd\nimport numpy as np\nimport time","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-08T08:42:30.858309Z","iopub.execute_input":"2025-01-08T08:42:30.858674Z","iopub.status.idle":"2025-01-08T08:42:30.863751Z","shell.execute_reply.started":"2025-01-08T08:42:30.858629Z","shell.execute_reply":"2025-01-08T08:42:30.862809Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def preprocess_data(data_path):\n    data = pd.read_csv(data_path)\n    data['label'] = data['Suicide'].map({'suicide': 1, 'non-suicide': 0})\n    data = data.dropna(subset=['Tweet', 'label'])\n    # Enhanced preprocessing\n    data['Tweet'] = data['Tweet'].str.lower()\n    data['Tweet'] = data['Tweet'].str.replace(r'http\\S+|www.\\S+', '', regex=True)  # Remove URLs\n    data['Tweet'] = data['Tweet'].str.replace(r'@\\w+', '@user', regex=True)  # Standardize mentions\n    data['Tweet'] = data['Tweet'].str.replace(r'[^\\w\\s]', '', regex=True)  # Remove special characters\n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T08:42:30.864646Z","iopub.execute_input":"2025-01-08T08:42:30.864906Z","iopub.status.idle":"2025-01-08T08:42:30.884786Z","shell.execute_reply.started":"2025-01-08T08:42:30.864871Z","shell.execute_reply":"2025-01-08T08:42:30.883832Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Dataset class with proper padding\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels, vocab, tokenizer, MAX_LENGTH = 512):\n        self.texts = texts\n        self.labels = labels\n        self.vocab = vocab\n        self.tokenizer = tokenizer\n        self.max_length = MAX_LENGTH\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        tokens = self.tokenizer(self.texts.iloc[idx])\n        indices = [self.vocab[token] for token in tokens]\n        \n        # Padding/truncating\n        if len(indices) < self.max_length:\n            indices = indices + [self.vocab['<pad>']] * (self.max_length - len(indices))\n        else:\n            indices = indices[:self.max_length]\n            \n        return torch.tensor(indices, dtype=torch.long), torch.tensor(self.labels.iloc[idx], dtype=torch.long)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T08:42:30.885616Z","iopub.execute_input":"2025-01-08T08:42:30.885850Z","iopub.status.idle":"2025-01-08T08:42:30.902663Z","shell.execute_reply.started":"2025-01-08T08:42:30.885823Z","shell.execute_reply":"2025-01-08T08:42:30.901730Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Enhanced model with LSTM\nclass TextClassificationModel(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, num_layers=2, dropout=0.5):\n        super(TextClassificationModel, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=1)\n        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, \n                           batch_first=True, bidirectional=True, dropout=dropout if num_layers > 1 else 0)\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(hidden_dim * 2, num_classes)  # *2 for bidirectional\n        \n    def forward(self, text):\n        embedded = self.embedding(text)\n        output, (hidden, cell) = self.lstm(embedded)\n        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n        hidden = self.dropout(hidden)\n        return self.fc(hidden)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T08:42:33.448551Z","iopub.execute_input":"2025-01-08T08:42:33.448885Z","iopub.status.idle":"2025-01-08T08:42:33.455438Z","shell.execute_reply.started":"2025-01-08T08:42:33.448859Z","shell.execute_reply":"2025-01-08T08:42:33.454333Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def evaluate(model, dataloader, criterion, device):\n    model.eval()\n    total_loss = 0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for texts, labels in dataloader:\n            texts, labels = texts.to(device), labels.to(device)\n            outputs = model(texts)\n            loss = criterion(outputs, labels)\n            \n            total_loss += loss.item()\n            preds = outputs.argmax(1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n    \n    return total_loss / len(dataloader), correct / total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T08:42:33.685698Z","iopub.execute_input":"2025-01-08T08:42:33.686037Z","iopub.status.idle":"2025-01-08T08:42:33.691754Z","shell.execute_reply.started":"2025-01-08T08:42:33.686010Z","shell.execute_reply":"2025-01-08T08:42:33.690695Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def train_model(model, train_dataloader, val_dataloader, criterion, optimizer, \n                device, num_epochs=25, patience=3):\n    best_val_loss = float('inf')\n    patience_counter = 0\n    \n    for epoch in range(num_epochs):\n        # Training\n        start_time = time.time()\n        model.train()\n        total_loss = 0\n        correct = 0\n        total = 0\n        \n        for texts, labels in train_dataloader:\n            texts, labels = texts.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(texts)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n            preds = outputs.argmax(1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n        \n        train_loss = total_loss / len(train_dataloader)\n        train_acc = correct / total\n        \n        # Validation\n        val_loss, val_acc = evaluate(model, val_dataloader, criterion, device)\n        # Calculate time elapsed\n        epoch_time = time.time() - start_time\n        \n        print(f'Epoch {epoch+1}/{num_epochs}:')\n        print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}')\n        print(f'Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}')\n        print(f'Time Elapsed: {epoch_time:.2f} seconds')\n        # Early stopping\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            torch.save(model.state_dict(), 'best_model.pt')\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print('Early stopping triggered')\n                break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T08:42:34.158749Z","iopub.execute_input":"2025-01-08T08:42:34.159056Z","iopub.status.idle":"2025-01-08T08:42:34.166345Z","shell.execute_reply.started":"2025-01-08T08:42:34.159034Z","shell.execute_reply":"2025-01-08T08:42:34.165391Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"EMBED_DIM = 256\nHIDDEN_DIM = 128\nBATCH_SIZE = 128\nLEARNING_RATE = 1e-4\nMAX_LENGTH = 512","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T08:42:39.336268Z","iopub.execute_input":"2025-01-08T08:42:39.336613Z","iopub.status.idle":"2025-01-08T08:42:39.340777Z","shell.execute_reply.started":"2025-01-08T08:42:39.336586Z","shell.execute_reply":"2025-01-08T08:42:39.339796Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T08:42:39.826937Z","iopub.execute_input":"2025-01-08T08:42:39.827256Z","iopub.status.idle":"2025-01-08T08:42:39.854259Z","shell.execute_reply.started":"2025-01-08T08:42:39.827230Z","shell.execute_reply":"2025-01-08T08:42:39.853431Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"data = preprocess_data('/kaggle/input/Suicide_Detection.csv')\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T08:42:41.778243Z","iopub.execute_input":"2025-01-08T08:42:41.778584Z","iopub.status.idle":"2025-01-08T08:42:51.227797Z","shell.execute_reply.started":"2025-01-08T08:42:41.778559Z","shell.execute_reply":"2025-01-08T08:42:51.226856Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                                               Tweet      Suicide  label\n0  ex wife threatening suiciderecently i left my ...      suicide      1\n1  am i weird i dont get affected by compliments ...  non-suicide      0\n2  finally 2020 is almost over so i can never hea...  non-suicide      0\n3          i need helpjust help me im crying so hard      suicide      1\n4  im so losthello my name is adam 16 and ive bee...      suicide      1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tweet</th>\n      <th>Suicide</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ex wife threatening suiciderecently i left my ...</td>\n      <td>suicide</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>am i weird i dont get affected by compliments ...</td>\n      <td>non-suicide</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>finally 2020 is almost over so i can never hea...</td>\n      <td>non-suicide</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>i need helpjust help me im crying so hard</td>\n      <td>suicide</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>im so losthello my name is adam 16 and ive bee...</td>\n      <td>suicide</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"train_texts, val_texts, train_labels, val_labels = train_test_split(\n        data['Tweet'], data['label'], test_size=0.2, random_state=42, stratify=data['label']\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T08:42:51.228935Z","iopub.execute_input":"2025-01-08T08:42:51.229165Z","iopub.status.idle":"2025-01-08T08:42:51.349627Z","shell.execute_reply.started":"2025-01-08T08:42:51.229146Z","shell.execute_reply":"2025-01-08T08:42:51.348777Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"tokenizer = get_tokenizer('basic_english')\nvocab = build_vocab_from_iterator(\n        map(tokenizer, train_texts),\n        specials=['<unk>', '<pad>'],\n        min_freq=2)\nvocab.set_default_index(vocab['<unk>'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T08:42:51.351178Z","iopub.execute_input":"2025-01-08T08:42:51.351538Z","iopub.status.idle":"2025-01-08T08:43:06.735876Z","shell.execute_reply.started":"2025-01-08T08:42:51.351498Z","shell.execute_reply":"2025-01-08T08:43:06.734863Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"train_dataset = TextDataset(train_texts, train_labels, vocab, tokenizer)\nval_dataset = TextDataset(val_texts, val_labels, vocab, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T08:43:06.737288Z","iopub.execute_input":"2025-01-08T08:43:06.737580Z","iopub.status.idle":"2025-01-08T08:43:06.741660Z","shell.execute_reply.started":"2025-01-08T08:43:06.737554Z","shell.execute_reply":"2025-01-08T08:43:06.740764Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T08:43:06.742757Z","iopub.execute_input":"2025-01-08T08:43:06.743064Z","iopub.status.idle":"2025-01-08T08:43:06.757497Z","shell.execute_reply.started":"2025-01-08T08:43:06.743032Z","shell.execute_reply":"2025-01-08T08:43:06.756619Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"model = TextClassificationModel(\n    vocab_size=len(vocab),\n    embed_dim=EMBED_DIM,\n    hidden_dim=HIDDEN_DIM,\n    num_classes=2\n).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T08:43:06.758411Z","iopub.execute_input":"2025-01-08T08:43:06.758733Z","iopub.status.idle":"2025-01-08T08:43:08.228190Z","shell.execute_reply.started":"2025-01-08T08:43:06.758710Z","shell.execute_reply":"2025-01-08T08:43:08.227413Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n\n# Train model\ntrain_model(model, train_dataloader, val_dataloader, criterion, optimizer, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-08T08:43:08.229070Z","iopub.execute_input":"2025-01-08T08:43:08.229357Z","iopub.status.idle":"2025-01-08T09:58:56.768486Z","shell.execute_reply.started":"2025-01-08T08:43:08.229322Z","shell.execute_reply":"2025-01-08T09:58:56.767463Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/25:\nTrain Loss: 0.3384 | Train Acc: 0.8558\nVal Loss: 0.2571 | Val Acc: 0.9034\nEpoch 2/25:\nTrain Loss: 0.2585 | Train Acc: 0.9044\nVal Loss: 0.2385 | Val Acc: 0.9083\nEpoch 3/25:\nTrain Loss: 0.2684 | Train Acc: 0.9048\nVal Loss: 0.2278 | Val Acc: 0.9139\nEpoch 4/25:\nTrain Loss: 0.2245 | Train Acc: 0.9173\nVal Loss: 0.2062 | Val Acc: 0.9233\nEpoch 5/25:\nTrain Loss: 0.1913 | Train Acc: 0.9296\nVal Loss: 0.1776 | Val Acc: 0.9329\nEpoch 6/25:\nTrain Loss: 0.1690 | Train Acc: 0.9380\nVal Loss: 0.1651 | Val Acc: 0.9377\nEpoch 7/25:\nTrain Loss: 0.1777 | Train Acc: 0.9348\nVal Loss: 0.1709 | Val Acc: 0.9362\nEpoch 8/25:\nTrain Loss: 0.1584 | Train Acc: 0.9429\nVal Loss: 0.1516 | Val Acc: 0.9431\nEpoch 9/25:\nTrain Loss: 0.1414 | Train Acc: 0.9491\nVal Loss: 0.1448 | Val Acc: 0.9463\nEpoch 10/25:\nTrain Loss: 0.1297 | Train Acc: 0.9533\nVal Loss: 0.1375 | Val Acc: 0.9487\nEpoch 11/25:\nTrain Loss: 0.1197 | Train Acc: 0.9572\nVal Loss: 0.1352 | Val Acc: 0.9507\nEpoch 12/25:\nTrain Loss: 0.1081 | Train Acc: 0.9609\nVal Loss: 0.1267 | Val Acc: 0.9530\nEpoch 13/25:\nTrain Loss: 0.0974 | Train Acc: 0.9653\nVal Loss: 0.1264 | Val Acc: 0.9558\nEpoch 14/25:\nTrain Loss: 0.0870 | Train Acc: 0.9690\nVal Loss: 0.1244 | Val Acc: 0.9554\nEpoch 15/25:\nTrain Loss: 0.0773 | Train Acc: 0.9726\nVal Loss: 0.1422 | Val Acc: 0.9515\nEpoch 16/25:\nTrain Loss: 0.0672 | Train Acc: 0.9767\nVal Loss: 0.1360 | Val Acc: 0.9553\nEpoch 17/25:\nTrain Loss: 0.0591 | Train Acc: 0.9793\nVal Loss: 0.1468 | Val Acc: 0.9556\nEarly stopping triggered\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}